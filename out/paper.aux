\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{nty/global//global/global}
\abx@aux@cite{0}{lu-image-aug}
\abx@aux@segm{0}{0}{lu-image-aug}
\abx@aux@cite{0}{ma-compression}
\abx@aux@segm{0}{0}{ma-compression}
\abx@aux@cite{0}{liu-gans}
\abx@aux@segm{0}{0}{liu-gans}
\abx@aux@cite{0}{tambouratzis-sorting}
\abx@aux@segm{0}{0}{tambouratzis-sorting}
\abx@aux@cite{0}{namphol-compression}
\abx@aux@segm{0}{0}{namphol-compression}
\abx@aux@cite{0}{liu-gans}
\abx@aux@segm{0}{0}{liu-gans}
\abx@aux@cite{0}{mao-deblurring}
\abx@aux@segm{0}{0}{mao-deblurring}
\abx@aux@cite{0}{ilesanmi-denoising}
\abx@aux@segm{0}{0}{ilesanmi-denoising}
\abx@aux@cite{0}{liu-gans}
\abx@aux@segm{0}{0}{liu-gans}
\abx@aux@cite{0}{liu-gans}
\abx@aux@segm{0}{0}{liu-gans}
\abx@aux@cite{0}{mao-deblurring}
\abx@aux@segm{0}{0}{mao-deblurring}
\abx@aux@cite{0}{liu-gans}
\abx@aux@segm{0}{0}{liu-gans}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\abx@aux@cite{0}{intro-pertub-theory}
\abx@aux@segm{0}{0}{intro-pertub-theory}
\abx@aux@cite{0}{shocher-ign}
\abx@aux@segm{0}{0}{shocher-ign}
\abx@aux@cite{0}{hirschfelder-dev-perturb}
\abx@aux@segm{0}{0}{hirschfelder-dev-perturb}
\abx@aux@cite{0}{intro-pertub-theory}
\abx@aux@segm{0}{0}{intro-pertub-theory}
\newlabel{eq:idem}{{1}{2}{Introduction}{equation.1.1}{}}
\newlabel{eq:idem-loss}{{2}{2}{Introduction}{equation.1.2}{}}
\newlabel{eq:g}{{3}{2}{Introduction}{equation.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Method}{2}{section.2}\protected@file@percent }
\newlabel{sec:method}{{2}{2}{Method}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}An idea from Perturbation Theory}{2}{subsection.2.1}\protected@file@percent }
\newlabel{def:near-idem}{{1}{2}{Near-idempotent to order $n$}{definition.1}{}}
\abx@aux@cite{0}{jordan-form}
\abx@aux@segm{0}{0}{jordan-form}
\newlabel{eq:ansatz}{{4}{3}{An idea from Perturbation Theory}{equation.2.4}{}}
\newlabel{eq:}{{5}{3}{An idea from Perturbation Theory}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Fixed Points and Stability Analysis}{3}{subsection.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Plot of $\mathbf  {K}' = 3 \mathbf  {K}^2 - 2 \mathbf  {K}^3$ in the case $\mathbf  {K}$ is scalar.}}{3}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:plot-g}{{1}{3}{Plot of $\vK ' = 3 \vK ^2 - 2 \vK ^3$ in the case $\vK $ is scalar}{figure.caption.2}{}}
\newlabel{eq:cond1}{{6}{3}{Fixed Points and Stability Analysis}{equation.2.6}{}}
\newlabel{eq:cond2}{{7}{3}{Fixed Points and Stability Analysis}{equation.2.7}{}}
\newlabel{eq:cond3}{{8}{3}{Fixed Points and Stability Analysis}{equation.2.8}{}}
\newlabel{eq:cond4}{{9}{3}{Fixed Points and Stability Analysis}{equation.2.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces 10-time recursive application of $h(\lambda ) = 3\lambda ^2 - 2\lambda ^3$ for each point on the complex plane. Black areas denote points converging onto $0$, while orange areas denote points converging onto $1$.}}{4}{figure.caption.3}\protected@file@percent }
\newlabel{fig:fractal}{{2}{4}{10-time recursive application of $h(\lambda ) = 3\lambda ^2 - 2\lambda ^3$ for each point on the complex plane. Black areas denote points converging onto $0$, while orange areas denote points converging onto $1$}{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Deriving a Training Scheme}{4}{subsection.2.3}\protected@file@percent }
\newlabel{eq:substitution}{{10}{4}{Deriving a Training Scheme}{equation.2.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experimental Results}{5}{section.3}\protected@file@percent }
\newlabel{sec:experiment}{{3}{5}{Experimental Results}{section.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Four neural networks for testing. Each ``Linear($n$, $m$)'' block is parameterized by its input dimension $n$ and its output dimension $m$, corresponding to the underlying $\mathbf  {W}\in \mathbb  {R}^{n \times m}$ weight matrix. Every block has an associated bias vector and LeakyReLU($0.2$) activation function. B1 represents a trivial network, B2 represents a relatively deep network, B3 represents a relatively wide network, and B4 represents a practically useful network.}}{5}{table.caption.4}\protected@file@percent }
\newlabel{tab:networks}{{1}{5}{Four neural networks for testing. Each ``Linear($n$, $m$)'' block is parameterized by its input dimension $n$ and its output dimension $m$, corresponding to the underlying $\vW \in \mathbb {R}^{n \times m}$ weight matrix. Every block has an associated bias vector and LeakyReLU($0.2$) activation function. B1 represents a trivial network, B2 represents a relatively deep network, B3 represents a relatively wide network, and B4 represents a practically useful network}{table.caption.4}{}}
\newlabel{fig:avg-abs-err}{{\caption@xref {fig:avg-abs-err}{ on input line 351}}{6}{Experimental Results}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Average of 10 runs of each algorithm for a variety of learning rates. Networks are randomly initialized and trained for $2\,500$ epochs. Runs which did not return a solution with lower idempotent error than the initial value are discarded, and the average is over remaining runs. For networks B3 and B4, learning rates $>0.22$ and $>0.52$ respectively had no runs with improvement in error. For ``Modified Backpropagation'' on B1, some runs resulted in approximately $\bm  {0}$ which, due to floating-point imprecision, results in the error spikes.}}{6}{figure.caption.5}\protected@file@percent }
\newlabel{fig:avg-epoch-err}{{\caption@xref {fig:avg-epoch-err}{ on input line 358}}{7}{Experimental Results}{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces On networks B2 and B3, the average idempotent error across 10 runs for each learning rate is reported for each algorithm. Each column of graphs represents one algorithm. ``Modified Backpropagation'' achieves lower idempotent error at lower learning rates than ``Ordinary Backpropagation''.}}{7}{figure.caption.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Application to Generative Networks}{7}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces U-DCGAN architecture trained on MNIST.}}{7}{figure.caption.7}\protected@file@percent }
\newlabel{fig:test}{{5}{7}{U-DCGAN architecture trained on MNIST}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Related Work}{8}{section.4}\protected@file@percent }
\newlabel{sec:related}{{4}{8}{Related Work}{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{8}{section.5}\protected@file@percent }
\newlabel{sec:conclusion}{{5}{8}{Conclusion}{section.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Solutions to the ansatz}{9}{appendix.A}\protected@file@percent }
\newlabel{app:solutions}{{A}{9}{Solutions to the ansatz}{appendix.A}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Jordan normal form analysis}{9}{appendix.B}\protected@file@percent }
\newlabel{app:jordan}{{B}{9}{Jordan normal form analysis}{appendix.B}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Automatic differentiation rule}{9}{appendix.C}\protected@file@percent }
\newlabel{app:autodiff-rule}{{C}{9}{Automatic differentiation rule}{appendix.C}{}}
\abx@aux@read@bbl@mdfivesum{6E518138ACCDBFBD4A246F01CBF272AA}
\abx@aux@defaultrefcontext{0}{jordan-form}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{hirschfelder-dev-perturb}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{ilesanmi-denoising}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{intro-pertub-theory}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{liu-gans}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{lu-image-aug}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{ma-compression}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{mao-deblurring}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{namphol-compression}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{shocher-ign}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{tambouratzis-sorting}{nty/global//global/global}
\gdef \@abspage@last{9}
