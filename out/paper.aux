\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{lu-image-aug}
\citation{ma-compression,liu-gans}
\citation{tambouratzis-sorting}
\citation{namphol-compression,liu-gans}
\citation{mao-deblurring,ilesanmi-denoising,liu-gans}
\citation{liu-gans}
\citation{mao-deblurring,liu-gans}
\newlabel{sec:intro}{{1}{1}{}{section.1}{}}
\newlabel{sec:intro@cref}{{[section][1][]1}{[1][1][]1}}
\newlabel{eq:idem}{{1}{1}{}{equation.1.1}{}}
\newlabel{eq:idem@cref}{{[equation][1][]1}{[1][1][]1}}
\citation{intro-pertub-theory}
\citation{shocher-ign}
\citation{hirschfelder-dev-perturb}
\citation{intro-pertub-theory}
\newlabel{eq:idem-loss}{{2}{2}{}{equation.1.2}{}}
\newlabel{eq:idem-loss@cref}{{[equation][2][]2}{[1][1][]2}}
\newlabel{eq:g}{{3}{2}{}{equation.1.3}{}}
\newlabel{eq:g@cref}{{[equation][3][]3}{[1][2][]2}}
\newlabel{sec:method}{{2}{2}{}{section.2}{}}
\newlabel{sec:method@cref}{{[section][2][]2}{[1][2][]2}}
\newlabel{def:near-idem}{{2.1}{2}{\textbf {Near-idempotent to order $n$}}{theorem.2.1}{}}
\newlabel{def:near-idem@cref}{{[definition][1][2]2.1}{[1][2][]2}}
\newlabel{eq:ansatz}{{4}{2}{}{equation.2.4}{}}
\newlabel{eq:ansatz@cref}{{[equation][4][]4}{[1][2][]2}}
\newlabel{eq:}{{5}{2}{}{equation.2.5}{}}
\newlabel{eq:@cref}{{[equation][5][]5}{[1][2][]2}}
\citation{jordan-form}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:plot-g}{{1}{3}{Plot of $\vK ' = 3 \vK ^2 - 2 \vK ^3$ in the case $\vK $ is scalar}{figure.caption.1}{}}
\newlabel{fig:plot-g@cref}{{[figure][1][]1}{[1][3][]3}}
\newlabel{eq:cond1}{{6}{3}{}{equation.2.6}{}}
\newlabel{eq:cond1@cref}{{[equation][6][]6}{[1][3][]3}}
\newlabel{eq:cond2}{{7}{3}{}{equation.2.7}{}}
\newlabel{eq:cond2@cref}{{[equation][7][]7}{[1][3][]3}}
\newlabel{eq:cond3}{{8}{3}{}{equation.2.8}{}}
\newlabel{eq:cond3@cref}{{[equation][8][]8}{[1][3][]3}}
\newlabel{eq:cond4}{{9}{3}{}{equation.2.9}{}}
\newlabel{eq:cond4@cref}{{[equation][9][]9}{[1][3][]3}}
\newlabel{sec:method-scheme}{{2.3}{3}{}{subsection.2.3}{}}
\newlabel{sec:method-scheme@cref}{{[subsection][3][2]2.3}{[1][3][]3}}
\newlabel{eq:substitution}{{10}{3}{}{equation.2.10}{}}
\newlabel{eq:substitution@cref}{{[equation][10][]10}{[1][3][]3}}
\newlabel{fig:fractal}{{2}{4}{10-time recursive application of $h(\lambda ) = 3\lambda ^2 - 2\lambda ^3$ for each point on the complex plane. Black areas denote points converging onto $0$, while orange areas denote points converging onto $1$}{figure.caption.2}{}}
\newlabel{fig:fractal@cref}{{[figure][2][]2}{[1][3][]4}}
\newlabel{sec:experiment}{{3}{4}{}{section.3}{}}
\newlabel{sec:experiment@cref}{{[section][3][]3}{[1][4][]4}}
\newlabel{tab:networks}{{1}{4}{Four neural networks for testing. Each ``Linear($n$, $m$)'' block is parameterized by its input dimension $n$ and its output dimension $m$, corresponding to the underlying $\vW \in \mathbb {R}^{m \times n}$ weight matrix. Every block has an associated bias vector and LeakyReLU($0.2$) activation function. B1 represents a trivial network, B2 represents a relatively deep network, B3 represents a relatively wide network, and B4 represents a more realistic network}{table.caption.3}{}}
\newlabel{tab:networks@cref}{{[table][1][]1}{[1][4][]4}}
\citation{li-visualizing}
\citation{li-visualizing}
\newlabel{sec:experiment-qual}{{3.1}{5}{}{subsection.3.1}{}}
\newlabel{sec:experiment-qual@cref}{{[subsection][1][3]3.1}{[1][5][]5}}
\newlabel{fig:pca-b2}{{3}{5}{Representative projections of the optimizer trajectories over 2500 epochs of either algorithm on the B2 model at optimal learning rates (fig. \ref {fig:avg-abs-err}). Total variance captured is $>97.8\%$ with cosine similarity of PC1 and PC2 less than $1.0\times 10^{-6}$. Optimizer trajectory of Modified Backpropagation deviates significantly from Ordinary Backpropagation}{figure.caption.4}{}}
\newlabel{fig:pca-b2@cref}{{[figure][3][]3}{[1][5][]5}}
\newlabel{fig:cos-b2}{{4}{5}{Absolute cosine similarity of gradients over time of a representative training run with model B2. ``Along OB'' optimizes the network with Ordinary Backpropagation and compare at each timepoint with suggested gradient from Modified Backpropagation. ``Along MB'' optimizes the network with Modified Backpropagation and compares with suggested gradient from Ordinary Backpropagation. ``Separate'' compares gradients of each optimizer as they independently optimize the network. Gradients suggested by Modified Backpropagation remains significantly different from those suggested by Ordinary Backpropagation}{figure.caption.5}{}}
\newlabel{fig:cos-b2@cref}{{[figure][4][]4}{[1][5][]5}}
\newlabel{fig:norm-b2}{{5}{5}{Norm of gradients over time of a representative training run with model B2. The network is optimized independently by either algorithm at optimal learning rates (fig. \ref {fig:avg-abs-err}). Modified Backpropagation gives consistently stronger gradient signal than Ordinary Backpropagation}{figure.caption.6}{}}
\newlabel{fig:norm-b2@cref}{{[figure][5][]5}{[1][5][]5}}
\newlabel{sec:experiment-quant}{{3.2}{5}{}{subsection.3.2}{}}
\newlabel{sec:experiment-quant@cref}{{[subsection][2][3]3.2}{[1][5][]5}}
\citation{shocher-ign}
\citation{shocher-ign}
\citation{shocher-ign}
\newlabel{fig:avg-abs-err}{{6}{6}{Average of 10 runs of each algorithm for a variety of learning rates. Networks are randomly initialized and trained for $2\,500$ epochs. Runs which did not return a network with lower idempotent error than the initial value are discarded, and the average is over remaining runs. For networks B3 and B4, learning rates $>0.22$ and $>0.52$ respectively had no runs with improvement in error. For Modified Backpropagation on B1, some runs resulted in approximately $\bm {0}$ which, due to floating-point imprecision, results in the error spikes}{figure.caption.7}{}}
\newlabel{fig:avg-abs-err@cref}{{[figure][6][]6}{[1][6][]6}}
\newlabel{sec:experiment-gen}{{3.3}{6}{}{subsection.3.3}{}}
\newlabel{sec:experiment-gen@cref}{{[subsection][3][3]3.3}{[1][6][]6}}
\newlabel{eq:gan-loss}{{11}{6}{}{equation.3.11}{}}
\newlabel{eq:gan-loss@cref}{{[equation][11][]11}{[1][6][]6}}
\citation{shocher-ign}
\citation{shocher-ign}
\citation{shocher-ign}
\citation{shocher-ign}
\citation{mikolov-rnn,le-rnn-relu}
\citation{arjovsky-rnn}
\citation{saxe-isometry,kiani-projunn,jing-tunable-unn}
\citation{ardizzone-inv}
\citation{shocher-ign}
\citation{spall-perturb}
\citation{scheinberg-approx,do-approx,bandler-approx}
\newlabel{fig:avg-epoch-err}{{7}{7}{On networks B2 and B3, the average idempotent error across 10 runs for each learning rate is reported for each algorithm. Each column of graphs represents one algorithm. Modified Backpropagation achieves lower idempotent error at lower learning rates than Ordinary Backpropagation. The biggest relative improvement between algorithms occurs in the first $\sim 500$ epochs}{figure.caption.8}{}}
\newlabel{fig:avg-epoch-err@cref}{{[figure][7][]7}{[1][6][]7}}
\newlabel{sec:related}{{4}{7}{}{section.4}{}}
\newlabel{sec:related@cref}{{[section][4][]4}{[1][7][]7}}
\citation{shocher-ign}
\bibdata{bibliography}
\bibcite{ardizzone-inv}{{1}{2019}{{Ardizzone et~al.}}{{Ardizzone, Kruse, Wirkert, Rahner, Pellegrini, Klessen, Maier-Hein, Rother, and Köthe}}}
\newlabel{fig:gen-mnist}{{8}{8}{Uncurated generations of U-DCGAN model trained on MNIST with Modified Backpropagation for optimizing idempotent and tightness losses. In the top row are samples drawn from a random distribution with mean $0$ and variance $1$, whist the second and third rows represent first and second application of the network to these samples, respectively}{figure.caption.9}{}}
\newlabel{fig:gen-mnist@cref}{{[figure][8][]8}{[1][7][]8}}
\newlabel{fig:latent-mnist}{{9}{8}{Latent space manipulation for U-DCGAN model trained on MNIST with Modified Backpropagation for optimizing idempotent and tightness losses. Samples $\mathbf {A}$ and $\mathbf {B}$ are selected randomly while remaining samples are linear combinations of these. We give the first and second application of the model}{figure.caption.10}{}}
\newlabel{fig:latent-mnist@cref}{{[figure][9][]9}{[1][7][]8}}
\newlabel{sec:conclusion}{{5}{8}{Alternatives to gradients}{section.5}{}}
\newlabel{sec:conclusion@cref}{{[section][5][]5}{[1][8][]8}}
\bibcite{arjovsky-rnn}{{2}{2016}{{Arjovsky et~al.}}{{Arjovsky, Shah, and Bengio}}}
\bibcite{bandler-approx}{{3}{1988}{{Bandler et~al.}}{{Bandler, Chen, Daijavad, and Madsen}}}
\bibcite{do-approx}{{4}{2013}{{Do \& Reynolds}}{{Do and Reynolds}}}
\bibcite{jordan-form}{{5}{2009}{{H.~Weintraub}}{{}}}
\bibcite{hirschfelder-dev-perturb}{{6}{1964}{{Hirschfelder et~al.}}{{Hirschfelder, Brown, and Epstein}}}
\bibcite{ilesanmi-denoising}{{7}{2021}{{Ilesanmi \& Ilesanmi}}{{Ilesanmi and Ilesanmi}}}
\bibcite{jing-tunable-unn}{{8}{2017}{{Jing et~al.}}{{Jing, Shen, Dubček, Peurifoy, Skirlo, LeCun, Tegmark, and Soljačić}}}
\bibcite{intro-pertub-theory}{{9}{1995}{{Kato}}{{}}}
\bibcite{kiani-projunn}{{10}{2022}{{Kiani et~al.}}{{Kiani, Balestriero, LeCun, and Lloyd}}}
\bibcite{le-rnn-relu}{{11}{2015}{{Le et~al.}}{{Le, Jaitly, and Hinton}}}
\bibcite{li-visualizing}{{12}{2018}{{Li et~al.}}{{Li, Xu, Taylor, Studer, and Goldstein}}}
\bibcite{liu-gans}{{13}{2021}{{Liu et~al.}}{{Liu, Huang, Yu, Wang, and Mallya}}}
\bibcite{lu-image-aug}{{14}{2022}{{Lu et~al.}}{{Lu, Chen, Olaniyi, and Huang}}}
\bibcite{ma-compression}{{15}{2020}{{Ma et~al.}}{{Ma, Zhang, Jia, Zhao, Wang, and Wang}}}
\bibcite{mao-deblurring}{{16}{2023}{{Mao et~al.}}{{Mao, Wan, Dai, and Yu}}}
\bibcite{mikolov-rnn}{{17}{2015}{{Mikolov et~al.}}{{Mikolov, Joulin, Chopra, Mathieu, and Ranzato}}}
\bibcite{namphol-compression}{{18}{1996}{{Namphol et~al.}}{{Namphol, Chin, and Arozullah}}}
\bibcite{saxe-isometry}{{19}{2014}{{Saxe et~al.}}{{Saxe, McClelland, and Ganguli}}}
\bibcite{scheinberg-approx}{{20}{2022}{{Scheinberg}}{{}}}
\bibcite{shocher-ign}{{21}{2023}{{Shocher et~al.}}{{Shocher, Dravid, Gandelsman, Mosseri, Rubinstein, and Efros}}}
\bibcite{spall-perturb}{{22}{1998}{{Spall}}{{}}}
\bibcite{tambouratzis-sorting}{{23}{1999}{{Tambouratzis}}{{}}}
\bibstyle{icml2025}
\newlabel{app:solutions}{{A}{11}{Impact Statement}{appendix.A}{}}
\newlabel{app:solutions@cref}{{[appendix][1][2147483647]A}{[1][11][]11}}
\newlabel{app:jordan}{{B}{12}{Impact Statement}{appendix.B}{}}
\newlabel{app:jordan@cref}{{[appendix][2][2147483647]B}{[1][12][]12}}
\newlabel{eq:k-problem}{{12}{12}{Impact Statement}{equation.B.12}{}}
\newlabel{eq:k-problem@cref}{{[equation][12][2147483647]12}{[1][12][]12}}
\newlabel{eq:j-problem}{{13}{12}{Impact Statement}{equation.B.13}{}}
\newlabel{eq:j-problem@cref}{{[equation][13][2147483647]13}{[1][12][]12}}
\newlabel{eq:app-cond1}{{14}{12}{Impact Statement}{equation.B.14}{}}
\newlabel{eq:app-cond1@cref}{{[equation][14][2147483647]14}{[1][12][]12}}
\newlabel{eq:app-cond2}{{15}{12}{Impact Statement}{equation.B.15}{}}
\newlabel{eq:app-cond2@cref}{{[equation][15][2147483647]15}{[1][12][]12}}
\newlabel{eq:app-cond3}{{16}{12}{Impact Statement}{equation.B.16}{}}
\newlabel{eq:app-cond3@cref}{{[equation][16][2147483647]16}{[1][12][]12}}
\newlabel{eq:app-cond4}{{17}{12}{Impact Statement}{equation.B.17}{}}
\newlabel{eq:app-cond4@cref}{{[equation][17][2147483647]17}{[1][12][]12}}
\newlabel{app:autodiff-rule}{{C}{13}{Impact Statement}{appendix.C}{}}
\newlabel{app:autodiff-rule@cref}{{[appendix][3][2147483647]C}{[1][13][]13}}
\newlabel{alg:rule}{{1}{13}{Modified Backpropagation PyTorch rule}{algorithm.1}{}}
\newlabel{alg:rule@cref}{{[algorithm][1][2147483647]1}{[1][13][]13}}
\newlabel{fig:mod_autodiff_graph}{{10(a)}{14}{Subfigure 10(a)}{subfigure.10.1}{}}
\newlabel{fig:mod_autodiff_graph@cref}{{[subfigure][1][2147483647,10]10(a)}{[1][14][]14}}
\newlabel{sub@fig:mod_autodiff_graph}{{(a)}{14}{Subfigure 10(a)\relax }{subfigure.10.1}{}}
\newlabel{fig:ord_autodiff_graph}{{10(b)}{14}{Subfigure 10(b)}{subfigure.10.2}{}}
\newlabel{fig:ord_autodiff_graph@cref}{{[subfigure][2][2147483647,10]10(b)}{[1][14][]14}}
\newlabel{sub@fig:ord_autodiff_graph}{{(b)}{14}{Subfigure 10(b)\relax }{subfigure.10.2}{}}
\newlabel{fig:comp-graphs}{{10}{14}{PyTorch computational graphs for gradient calculation}{figure.caption.15}{}}
\newlabel{fig:comp-graphs@cref}{{[figure][10][2147483647]10}{[1][14][]14}}
\newlabel{app:test-networks-data}{{D}{15}{Impact Statement}{appendix.D}{}}
\newlabel{app:test-networks-data@cref}{{[appendix][4][2147483647]D}{[1][15][]15}}
\newlabel{fig:error-test-nets-1}{{11(a)}{15}{Subfigure 11(a)}{subfigure.11.1}{}}
\newlabel{fig:error-test-nets-1@cref}{{[subfigure][1][2147483647,11]11(a)}{[1][15][]15}}
\newlabel{sub@fig:error-test-nets-1}{{(a)}{15}{Subfigure 11(a)\relax }{subfigure.11.1}{}}
\newlabel{fig:error-test-nets-2}{{11(b)}{15}{Subfigure 11(b)}{subfigure.11.2}{}}
\newlabel{fig:error-test-nets-2@cref}{{[subfigure][2][2147483647,11]11(b)}{[1][15][]15}}
\newlabel{sub@fig:error-test-nets-2}{{(b)}{15}{Subfigure 11(b)\relax }{subfigure.11.2}{}}
\newlabel{fig:error-test-nets-3}{{11(c)}{15}{Subfigure 11(c)}{subfigure.11.3}{}}
\newlabel{fig:error-test-nets-3@cref}{{[subfigure][3][2147483647,11]11(c)}{[1][15][]15}}
\newlabel{sub@fig:error-test-nets-3}{{(c)}{15}{Subfigure 11(c)\relax }{subfigure.11.3}{}}
\newlabel{fig:error-test-nets-4}{{11(d)}{15}{Subfigure 11(d)}{subfigure.11.4}{}}
\newlabel{fig:error-test-nets-4@cref}{{[subfigure][4][2147483647,11]11(d)}{[1][15][]15}}
\newlabel{sub@fig:error-test-nets-4}{{(d)}{15}{Subfigure 11(d)\relax }{subfigure.11.4}{}}
\newlabel{fig:error-test-nets}{{11}{15}{For each network we report the distribution of \textbf {absolute idempotent error} after 2500 epochs of training 250 randomly initialized networks. Note that distributions are narrow for each algorithm and often separated by more than an order of magnitude}{figure.caption.16}{}}
\newlabel{fig:error-test-nets@cref}{{[figure][11][2147483647]11}{[1][15][]15}}
\newlabel{fig:norm-test-nets-1}{{12(a)}{16}{Subfigure 12(a)}{subfigure.12.1}{}}
\newlabel{fig:norm-test-nets-1@cref}{{[subfigure][1][2147483647,12]12(a)}{[1][15][]16}}
\newlabel{sub@fig:norm-test-nets-1}{{(a)}{16}{Subfigure 12(a)\relax }{subfigure.12.1}{}}
\newlabel{fig:norm-test-nets-2}{{12(b)}{16}{Subfigure 12(b)}{subfigure.12.2}{}}
\newlabel{fig:norm-test-nets-2@cref}{{[subfigure][2][2147483647,12]12(b)}{[1][15][]16}}
\newlabel{sub@fig:norm-test-nets-2}{{(b)}{16}{Subfigure 12(b)\relax }{subfigure.12.2}{}}
\newlabel{fig:norm-test-nets-3}{{12(c)}{16}{Subfigure 12(c)}{subfigure.12.3}{}}
\newlabel{fig:norm-test-nets-3@cref}{{[subfigure][3][2147483647,12]12(c)}{[1][15][]16}}
\newlabel{sub@fig:norm-test-nets-3}{{(c)}{16}{Subfigure 12(c)\relax }{subfigure.12.3}{}}
\newlabel{fig:norm-test-nets-4}{{12(d)}{16}{Subfigure 12(d)}{subfigure.12.4}{}}
\newlabel{fig:norm-test-nets-4@cref}{{[subfigure][4][2147483647,12]12(d)}{[1][15][]16}}
\newlabel{sub@fig:norm-test-nets-4}{{(d)}{16}{Subfigure 12(d)\relax }{subfigure.12.4}{}}
\newlabel{fig:norm-test-nets}{{12}{16}{For each network we report the distribution of \textbf {norms} after 2500 epochs of training 250 randomly initialized networks. Note that distributions for either method are not centred around zero, indicating that the trained network is a non-trivial idempotent mapping. To calculate the norm we pass each canonical basis vector of $\mathbb {R}^n$ as input by concatenating them into the identity matrix and report here the Frobenius norm of the output matrix}{figure.caption.17}{}}
\newlabel{fig:norm-test-nets@cref}{{[figure][12][2147483647]12}{[1][15][]16}}
\citation{shocher-ign}
\citation{shocher-ign}
\newlabel{app:gen-training-scheme}{{E}{17}{Impact Statement}{appendix.E}{}}
\newlabel{app:gen-training-scheme@cref}{{[appendix][5][2147483647]E}{[1][17][]17}}
\newlabel{tab:scheme}{{2}{17}{Network architecture}{table.caption.18}{}}
\newlabel{tab:scheme@cref}{{[table][2][2147483647]2}{[1][17][]17}}
\newlabel{tab:parameters}{{3}{17}{Training parameters}{table.caption.19}{}}
\newlabel{tab:parameters@cref}{{[table][3][2147483647]3}{[1][17][]17}}
\newlabel{app:gen-samples}{{F}{18}{Impact Statement}{appendix.F}{}}
\newlabel{app:gen-samples@cref}{{[appendix][6][2147483647]F}{[1][18][]18}}
\newlabel{fig:big-gen-mnist}{{13}{18}{Uncurated generations of U-DCGAN model trained on MNIST with Modified Backpropagation for optimizing idempotent and tightness losses. All samples are drawn from a random distribution with mean $0$ and variance $1$, and the result of applying the network is shown}{figure.caption.20}{}}
\newlabel{fig:big-gen-mnist@cref}{{[figure][13][2147483647]13}{[1][18][]18}}
\newlabel{app:pca}{{G}{19}{Impact Statement}{appendix.G}{}}
\newlabel{app:pca@cref}{{[appendix][7][2147483647]G}{[1][19][]19}}
\newlabel{fig:big-pca}{{14}{19}{Representative projections of the optimizer trajectories over 2500 epochs of either algorithm on each of the B1-B4 models at optimal learning rates (fig. \ref {fig:avg-abs-err}). Total variance captured is $>90\%$ with cosine similarity of PC1 and PC2 less than $1.0\times 10^{-6}$ across all plots. Optimizer trajectory of Modified Backpropagation often deviates significantly from Ordinary Backpropagation, but may sometimes overlap (e.g., B4)}{figure.caption.21}{}}
\newlabel{fig:big-pca@cref}{{[figure][14][2147483647]14}{[1][19][]19}}
\gdef \@abspage@last{19}
