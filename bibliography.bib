@article{lu-image-aug,
  title    = {{G}enerative {A}dversarial {N}etworks ({G}{A}{N}s) for image augmentation in agriculture: A systematic review},
  journal  = {Computers and Electronics in Agriculture},
  volume   = {200},
  pages    = {107208},
  year     = {2022},
  issn     = {0168-1699},
  doi      = {https://doi.org/10.1016/j.compag.2022.107208},
  url      = {https://www.sciencedirect.com/science/article/pii/S0168169922005233},
  author   = {Yuzhen Lu and Dong Chen and Ebenezer Olaniyi and Yanbo Huang},
  keywords = {GAN, Image Augmentation, Agriculture, Deep Learning, Computer Vision},
  abstract = {In agricultural image analysis, optimal model performance is keenly pursued for better fulfilling visual recognition tasks (e.g., image classification, segmentation, object detection and localization), in the presence of challenges with biological variability and unstructured environments. Large-scale, balanced and ground-truthed image datasets are tremendously beneficial but most often difficult to obtain to fuel the development of highly performant models. As artificial intelligence through deep learning is impacting analysis and modeling of agricultural images, image augmentation plays a crucial role in boosting model performance while reducing manual efforts for image collection and labelling, by algorithmically creating and expanding datasets. Beyond traditional data augmentation techniques, generative adversarial network (GAN) invented in 2014 in the computer vision community, provides a suite of novel approaches that can learn good data representations and generate highly realistic samples. Since 2017, there has been a growth of research into GANs for image augmentation or synthesis in agriculture for improved model performance. This paper presents an overview of the evolution of GAN architectures followed by a first systematic review of various applications in agriculture and food systems (https://github.com/Derekabc/GANs-Agriculture), involving a diversity of visual recognition tasks for plant health conditions, weeds, fruits (preharvest), aquaculture, animal farming, plant phenotyping as well as postharvest detection of fruit defects. Challenges and opportunities of GANs are discussed for future research.}
}

@article{ma-compression,
  author   = {Ma, Siwei and Zhang, Xinfeng and Jia, Chuanmin and Zhao, Zhenghui and Wang, Shiqi and Wang, Shanshe},
  journal  = {IEEE Transactions on Circuits and Systems for Video Technology},
  title    = {{I}mage and {V}ideo {C}ompression {W}ith {N}eural {N}etworks: {A} {R}eview},
  year     = {2020},
  volume   = {30},
  number   = {6},
  pages    = {1683-1698},
  keywords = {Image coding;Biological neural networks;Video coding;Video compression;Redundancy;Transform coding;Neural network;deep learning;CNN;image compression;video coding},
  doi      = {10.1109/TCSVT.2019.2910119}
}

@article{liu-gans,
  author   = {Liu, Ming-Yu and Huang, Xun and Yu, Jiahui and Wang, Ting-Chun and Mallya, Arun},
  journal  = {Proceedings of the IEEE},
  title    = {{G}enerative {A}dversarial {N}etworks for {I}mage and {V}ideo {S}ynthesis: {A}lgorithms and {A}pplications},
  year     = {2021},
  volume   = {109},
  number   = {5},
  pages    = {839-862},
  keywords = {Computer vision;Generators;Training data;Generative adversarial networks;Linear programming;Visualization;Training data;Neural networks;Photorealism;Image synthesis;Rendering (computer graphics);Computer vision;generative adversarial networks (GANs);image and video synthesis;image processing;neural rendering},
  doi      = {10.1109/JPROC.2021.3049196}
}

@article{tambouratzis-sorting,
  author   = {Tambouratzis, T.},
  journal  = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  title    = {{A} novel artificial neural network for sorting},
  year     = {1999},
  volume   = {29},
  number   = {2},
  pages    = {271-275},
  keywords = {Artificial neural networks;Sorting;Simulated annealing;Clamps;Signal resolution;Computer networks;Very large scale integration;Process design;Signal design;Digital signal processing},
  doi      = {10.1109/3477.752799}
}

@article{ilesanmi-denoising,
  author  = {Ilesanmi, Ademola E.
             and Ilesanmi, Taiwo O.},
  title   = {{M}ethods for image denoising using convolutional neural network: a review},
  journal = {Complex {\&} Intelligent Systems},
  year    = {2021},
  month   = {10},
  day     = {01},
  volume  = {7},
  number  = {5},
  pages   = {2179-2198},
  issn    = {2198-6053},
  doi     = {10.1007/s40747-021-00428-4},
  url     = {https://doi.org/10.1007/s40747-021-00428-4}
}

@article{namphol-compression,
  author   = {Namphol, A. and Chin, S.H. and Arozullah, M.},
  journal  = {IEEE Transactions on Aerospace and Electronic Systems},
  title    = {{I}mage compression with a hierarchical neural network},
  year     = {1996},
  volume   = {32},
  number   = {1},
  pages    = {326-338},
  keywords = {Image coding;Neural networks;Data compression;Wavelet transforms;Books;Image storage;Image analysis;Training data;Layout;Biological neural networks},
  doi      = {10.1109/7.481272}
}

@article{mao-deblurring,
  author   = {Mao, Yuxin and Wan, Zhexiong and Dai, Yuchao and Yu, Xin},
  journal  = {IEEE Transactions on Circuits and Systems for Video Technology},
  title    = {{D}eep {I}dempotent {N}etwork for {E}fficient {S}ingle {I}mage {B}lind {D}eblurring},
  year     = {2023},
  volume   = {33},
  number   = {1},
  pages    = {172-185},
  keywords = {Image restoration;Real-time systems;Deep learning;Mathematical models;Kernel;Training;Convolutional neural networks;Idempotent network;single image blind deblurring;efficient deblurring},
  doi      = {10.1109/TCSVT.2022.3202361}
}

@book{intro-pertub-theory,
  title     = {{P}erturbation {T}heory for {L}inear {O}perators},
  author    = {Kato, Tosio},
  year      = {1995},
  publisher = {Springer},
  isbn      = {978-981-16-2527-5},
  doi       = {https://doi.org/10.1007/978-981-16-2528-2}
}

@inproceedings{shocher-ign,
  title         = {{I}dempotent {G}enerative {N}etwork},
  author        = {Assaf Shocher and Amil Dravid and Yossi Gandelsman and Inbar Mosseri and Michael Rubinstein and Alexei A. Efros},
  booktitle     = {The Twelfth International Conference on Learning Representations},
  year          = {2023},
  eprint        = {2311.01462},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2311.01462}
}

@incollection{hirschfelder-dev-perturb,
  title     = {{R}ecent {D}evelopments in {P}erturbation {T}heory},
  editor    = {Per-Olov Löwdin},
  series    = {Advances in Quantum Chemistry},
  publisher = {Academic Press},
  volume    = {1},
  pages     = {255-374},
  year      = {1964},
  issn      = {0065-3276},
  doi       = {https://doi.org/10.1016/S0065-3276(08)60381-0},
  author    = {Joseph O. Hirschfelder and W. Byers Brown and Saul T. Epstein},
  abstract  = {Publisher Summary
               The purpose of this chapter is to provide information on the recent developments in perturbation theory. In recent years, there is a great increase of interest in the application of perturbation theory to the fundamental problems of quantum chemistry. Perturbation theory is designed to deal systematically with the effects of small perturbations on physical systems when the effects of the perturbations are mathematically too difficult to calculate exactly, and the properties of the unperturbed system are known. The new applications have been mainly to atoms where the reciprocal of the atomic number, l/Z, provides a natural perturbation parameter. These may be divided into two groups. The first consists of calculations of energy levels, and is a natural outgrowth of Hylleraas's classic work on the 1/Z expansion for two-electron atoms. The applications in the second group are to the calculation of expectation values and other properties of atoms and molecules, and are of much more recent origin. There are two principal reasons for the success of these new applications: (1) sufficient accuracy is frequently obtained from knowledge of a first-order perturbed wave function, and (2) a great advantage of perturbation theory is that the functional form of the perturbed wave function is shaped by the perturbation itself.}
}

@book{jordan-form,
  title     = {{J}ordan {C}anonical {F}orm},
  pages     = {1-24},
  subtitle  = {Theory and Practice},
  author    = {H. Weintraub, Steven},
  year      = {2009},
  publisher = {Springer}
}


@inproceedings{li-visualizing,
  author    = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  title     = {{V}isualizing the loss landscape of neural nets},
  year      = {2018},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages     = {6391-6401},
  numpages  = {11},
  location  = {Montr\'{e}al, Canada},
  series    = {NIPS'18}
}

@inproceedings{mikolov-rnn,
  title         = {{L}earning {L}onger {M}emory in {R}ecurrent {N}eural {N}etworks},
  author        = {Tomas Mikolov and Armand Joulin and Sumit Chopra and Michael Mathieu and Marc'Aurelio Ranzato},
  booktitle     = {Workshop Submission at ICLR 2015},
  year          = {2015},
  eprint        = {1412.7753},
  archiveprefix = {arXiv},
  primaryclass  = {cs.NE},
  url           = {https://arxiv.org/abs/1412.7753}
}

@misc{saxe-isometry,
  title         = {{E}xact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author        = {Andrew M. Saxe and James L. McClelland and Surya Ganguli},
  year          = {2014},
  eprint        = {1312.6120},
  archiveprefix = {arXiv},
  primaryclass  = {cs.NE},
  url           = {https://arxiv.org/abs/1312.6120}
}

@inproceedings{arjovsky-rnn,
  author    = {Arjovsky, Martin and Shah, Amar and Bengio, Yoshua},
  title     = {{U}nitary {E}volution {R}ecurrent {N}eural {N}etworks},
  year      = {2016},
  publisher = {JMLR.org},
  booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
  pages     = {1120-1128},
  numpages  = {9},
  location  = {New York, NY, USA},
  series    = {ICML'16}
}

@inproceedings{ardizzone-inv,
  title         = {{A}nalyzing {I}nverse {P}roblems with {I}nvertible {N}eural {N}etworks},
  author        = {Lynton Ardizzone and Jakob Kruse and Sebastian Wirkert and Daniel Rahner and Eric W. Pellegrini and Ralf S. Klessen and Lena Maier-Hein and Carsten Rother and Ullrich Köthe},
  booktitle     = {International Conference on Learning Representations},
  year          = {2019},
  eprint        = {1808.04730},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1808.04730}
}

@misc{le-rnn-relu,
  title         = {{A} {S}imple {W}ay to {I}nitialize {R}ecurrent {N}etworks of {R}ectified {L}inear {U}nits},
  author        = {Quoc V. Le and Navdeep Jaitly and Geoffrey E. Hinton},
  year          = {2015},
  eprint        = {1504.00941},
  archiveprefix = {arXiv},
  primaryclass  = {cs.NE},
  url           = {https://arxiv.org/abs/1504.00941}
}

@inproceedings{kiani-projunn,
  author    = {Kiani, Bobak and Balestriero, Randall and LeCun, Yann and Lloyd, Seth},
  booktitle = {{A}dvances in {N}eural {I}nformation {P}rocessing {S}ystems},
  editor    = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages     = {14448--14463},
  publisher = {Curran Associates, Inc.},
  title     = {proj{U}{N}{N}: efficient method for training deep networks with unitary matrices},
  volume    = {35},
  year      = {2022}
}


@inproceedings{jing-tunable-unn,
  author    = {Jing, Li and Shen, Yichen and Dubcek, Tena and Peurifoy, John and Skirlo, Scott and LeCun, Yann and Tegmark, Max and Solja\v{c}i\'{c}, Marin},
  title     = {{T}unable {E}fficient {U}nitary {N}eural {N}etworks ({E}{U}{N}{N}) and their application to {R}{N}{N}s},
  year      = {2017},
  publisher = {JMLR.org},
  abstract  = {Using unitary (instead of general) matrices in artificial neural networks (ANNs) is a promising way to solve the gradient explosion/vanishing problem, as well as to enable ANNs to learn long-term correlations in the data. This approach appears particularly promising for Recurrent Neural Networks (RNNs). In this work, we present a new architecture for implementing an Efficient Unitary Neural Network (EUNNs); its main advantages can be summarized as follows. Firstly, the representation capacity of the unitary space in an EUNN is fully tunable, ranging from a subspace of SU(N) to the entire unitary space. Secondly, the computational complexity for training an EUNN is merely O(1) per parameter. Finally, we test the performance of EUNNs on the standard copying task, the pixel-permuted MNIST digit recognition benchmark as well as the Speech Prediction Test (TIMIT). We find that our architecture significantly outperforms both other state-of-the-art unitary RNNs and the LSTM architecture, in terms of the final performance and/or the wall-clock training speed. EUNNs are thus promising alternatives to RNNs and LSTMs for a wide variety of applications.},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
  pages     = {1733-1741},
  numpages  = {9},
  location  = {Sydney, NSW, Australia},
  series    = {ICML'17}
}

@inproceedings{spall-perturb,
  author    = {Spall, James C.},
  title     = {{S}tochastic optimization and the simultaneous perturbation method},
  year      = {1999},
  isbn      = {0780357809},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/324138.324170},
  doi       = {10.1145/324138.324170},
  booktitle = {Proceedings of the 31st Conference on Winter Simulation: Simulation---a Bridge to the Future - Volume 1},
  pages     = {10-109},
  numpages  = {9},
  location  = {Phoenix, Arizona, USA},
  series    = {WSC '99}
}

@article{scheinberg-approx,
  title     = {{F}inite difference gradient approximation: {T}o randomize or not?},
  author    = {Scheinberg, Katya},
  journal   = {INFORMS Journal on Computing},
  volume    = {34},
  number    = {5},
  pages     = {2384--2388},
  year      = {2022},
  publisher = {INFORMS}
}

@article{do-approx,
  title     = {{T}heoretical connections between optimization algorithms based on an approximate gradient},
  author    = {Do, Sy T and Reynolds, Albert C},
  journal   = {Computational Geosciences},
  volume    = {17},
  pages     = {959--973},
  year      = {2013},
  publisher = {Springer}
}

@article{bandler-approx,
  title     = {{E}fficient optimization with integrated gradient approximations},
  author    = {Bandler, John W and Chen, Shao Hua and Daijavad, Shahrokh and Madsen, Kaj},
  journal   = {IEEE transactions on microwave theory and techniques},
  volume    = {36},
  number    = {2},
  pages     = {444--455},
  year      = {1988},
  publisher = {IEEE}
}

@inproceedings{song-subspaces,
  title         = {Does {S}{G}{D} really happen in tiny subspaces?},
  author        = {Minhak Song and Kwangjun Ahn and Chulhee Yun},
  booktitle     = {Proceedings of the Workshop on High-dimensional Learning Dynamics (HiLD) at ICML 2024},
  year          = {2024},
  eprint        = {2405.16002},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  publisher     = {PMLR},
  url           = {https://arxiv.org/abs/2405.16002}
}